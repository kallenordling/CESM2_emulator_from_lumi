import os
import random
from typing import Any
from functools import lru_cache

from omegaconf import OmegaConf
import torch
import numpy as np
import xarray as xr
from torch.utils.data import Dataset, DataLoader
from accelerate import Accelerator
import dask

# Constants for the minimum and maximum of our datasets
MIN_MAX_CONSTANTS = {"TREFHT": (-85.0, 60.0), "pr": (0.0, 6.0)}

# Convert from kelvin to celsius and from kg/m^2/s to mm/day
PREPROCESS_FN = {"TREFHT": lambda x: x - 273.15, "pr": lambda x: x * 86400}
fit_minmax = lambda x: (np.nanmin(x), np.nanmax(x))
# Normalization and Inverse Normalization functions
NORM_FN = {
    "TREFHT": lambda x: (x - 4.5) / 21.0,
    "pr": lambda x: np.cbrt(x),
}
DENORM_FN = {
    "TREFHT": lambda x: x * 21.0 + 4.5,
    "pr": lambda x: x**3,
}

# These functions transform the range of the data to [-1, 1]
MIN_MAX_FN = {"TREFHT": lambda x: x}


def min_max_norm(x: Any, min_val: float, max_val: float) -> Any:
    """Normalizes a data array to the range [-1, 1]"""
    return (x - min_val) / (max_val - min_val)


def min_max_denorm(x: Any, min_val: float, max_val: float) -> Any:
    """Inverse normalizes a data array from the range [-1, 1] to [min_val, max_val]"""
    return x * (max_val - min_val) + min_val


def preprocess(ds: xr.DataArray) -> xr.DataArray:
    """Preprocesses a data array"""

    # The name of the variable is contained within the dataarray
    return PREPROCESS_FN[ds.name](ds)


EMISSIONS_PATH = "/scratch/project_462001112/emulator_data/emissions_new.nc"

def scale_emis_0_1_log10(da: xr.DataArray, low_pct=1.0, high_pct=99.0, floor=1e-30):
    # TOMCAT emissions: non-negative
    x = da.clip(min=0)
    # avoid log(0)
    x = xr.where(x > 0, x, floor)

    lx = np.log10(x)

    lo = lx.quantile(low_pct/100.0, skipna=True)
    hi = lx.quantile(high_pct/100.0, skipna=True)

    z = (lx - lo) / (hi - lo)
    return z.clip(0, 1).fillna(0).astype("float32")

def scale_emis_m1_p1_log10(da: xr.DataArray, low_pct=1.0, high_pct=99.0, floor=1e-30):
    z01 = scale_emis_0_1_log10(da, low_pct, high_pct, floor)
    return (2.0 * z01 - 1.0).astype("float32")

@lru_cache(maxsize=1)
def _get_emissions_minmax():
    """
    Lataa emissions.nc vain kerran ja palauttaa min/max-arvot
    CO2_em_anthro:lle ja sul:lle.
    """
    ds_emis = xr.open_dataset(EMISSIONS_PATH)

    minmax = {}
    for var in ["CO2", "SO2"]:
        da = ds_emis[var]
        min_val = float(da.min())
        max_val = float(da.max())
        minmax[var] = (min_val, max_val)

    ds_emis.close()
    return minmax


def normalize(ds: xr.DataArray) -> xr.DataArray:
    """Normalizes a data array"""

    #print(f"[NORM DEBUG] ds.name={ds.name!r}, shape={ds.shape}, "
    #      f"min={float(ds.min(skipna=True)):.4f}, max={float(ds.max(skipna=True)):.4f}")

    if ds.name in ["CO2", "SO2"]:
        # Log-scale normalization to [-1, 1] for emissions
        result = scale_emis_m1_p1_log10(ds, low_pct=1.0, high_pct=99.5).fillna(0)
        #print(f"[NORM DEBUG] {ds.name} after norm: "
        #      f"min={float(result.min()):.4f}, max={float(result.max()):.4f}")
        return result

    # Other variables use predefined normalization functions
    norm = NORM_FN[ds.name](ds)
    return norm.fillna(0)

def denorm(ds: xr.DataArray) -> xr.DataArray:
    norm = DENORM_FN[ds.name](ds)

    min_val, max_val = MIN_MAX_CONSTANTS[ds.name]
    # norm = min_max_denorm(norm, min_val, max_val)
    return norm


class ClimateDataset(Dataset):
    def __init__(
        self,
        seq_len: int,
        realizations: list[str],
        data_dir: str,
        target_vars: list[str],
        cond_file: str,
        cond_vars: list[str],
    ):
        self.seq_len = seq_len
        self.realizations = realizations

        self.data_dir = data_dir

        # Necessary to convert vars into a Python list
        self.vars = OmegaConf.to_object(target_vars) if not isinstance(target_vars, list) else target_vars
        self.cond_vars = OmegaConf.to_object(cond_vars) if not isinstance(cond_vars, list) else cond_vars
        # Store one dataset (out of memory) as an xarray dataset for metadata
        # Store a different dataset as a torch tensor for speed
        self.xr_data: xr.Dataset
        self.tensor_data: torch.Tensor
        self.cond_file=cond_file
        # Load an example realization right off the bat
        #print('load_data')
        self.load_data(self.realizations[0])
        self.lats=0

    def estimate_num_batches(self, batch_size: int) -> int:
        """Estimates the number of batches in the dataset."""
        return len(self) * len(self.realizations) // batch_size

    def load_data(self, realization: str):
        """Loads the data from the spe
        cified paths and returns it as an xarray Dataset."""

        realization_dir = os.path.join(self.data_dir, realization, "*.nc")

        # Open up the dataset and make sure it's sorted by time
        #print(realization_dir)
        hist_years = list(range(1850, 2015, 5))  # every 5th year
        future_years = list(range(2015, 2101))  # every year
        selected_years = hist_years + future_years
        #xr_data = xr_data.sel(year=selected_years)
        dataset = xr.open_mfdataset(realization_dir, combine="by_coords").sortby("year")#.sel(year=selected_years)
        self.lats=dataset.lat
        # Only select the variables we are interested in
        dataset = dataset[self.vars]

        # Apply preprocessing and normalization
        self.xr_data = dataset.map(preprocess).map(normalize)

        #if self.spatial_resolution is not None:
        #    with dask.config.set(**{'array.slicing.split_large_chunks' : False}):
        #        self.xr_data = self.xr_data.coarsen(lon=3, lat=2).mean()

        self.tensor_data = self.convert_xarray_to_tensor(self.xr_data)
        cond_file=os.path.join(self.data_dir, self.cond_file)
        self.dataset_cond =xr.open_dataset(cond_file)#.sel(year=selected_years)
        self.dataset_cond = self.dataset_cond[self.cond_vars]
        #print(self.dataset_cond)
        self.dataset_cond = self.dataset_cond.map(normalize)



        self.tensor_data_cond = self.convert_xarray_to_tensor(self.dataset_cond)
        #print(self.tensor_data_cond.shape,'cond shape')
        #print(self.tensor_data.shape,'target_shape')
    def convert_xarray_to_tensor(self, ds: xr.Dataset) -> torch.Tensor:
        """Generate a tensor of data from an xarray dataset"""
        #print(ds)
        # Stacks the data variables ('pr', 'tas', ...) into a single dimension
        stacked_ds = ds.to_stacked_array(
            new_dim="var", sample_dims=["year", "lon", "lat"]
        ).transpose("var", "year", "lat", "lon")
        #print(stacked_ds.to_numpy())
        # Convert the numpy array to a torch tensor
        tensor_data = torch.tensor(stacked_ds.to_numpy(), dtype=torch.float32)

        return tensor_data
    def get_cond_from_coords(self, coord_dict):
        years = coord_dict["year"]
        # select those years from the conditioning dataset
        ds = self.dataset_cond.sel(year=years)
        return self.convert_xarray_to_tensor(ds)

    def convert_tensor_to_xarray(
        self, tensor: torch.Tensor, coords: xr.DataArray = None
    ) -> xr.Dataset:
        """Generate an xarray dataset from a tensor of data"""

        assert len(tensor.shape) == 4, "Tensor must have shape (var, time, lat, lon)"

        np_data = tensor.cpu().numpy()

        # Convert the numpy array to a dictionary of xr.DataArrays
        # with the same names as the original dataset
        data_vars = {
            var_name: (["time", "lat", "lon"], np_data[i])
            for i, var_name in enumerate(self.xr_data.data_vars.keys())
        }

        # Create the dataset with the same coordinates as the original dataset
        # Note: The original time values are lost and just start at 0 instead
        ds = xr.Dataset(
            data_vars,
            coords={
                "time": np.arange(np_data.shape[1]),
                "lat": np.linspace(-90, 90, np_data.shape[2]),
                "lon": np.linspace(0, 360, np_data.shape[3]),
            },
        ).map(denorm)

        # If we are provided time coords, create a new time coordinate
        if coords is not None:
            ds = ds.assign_coords(coords)
        return ds

    def __len__(self):
        return len(self.xr_data.year) - self.seq_len + 1

    def __getitem__(self, idx: int):
        """Defines how to get a specific index from the dataset"""
        return self.tensor_data[:, idx : idx + self.seq_len],self.tensor_data_cond[:, idx : idx + self.seq_len]


class ClimateDataLoader:
    def __init__(
        self,
        dataset: ClimateDataset,
        accelerator: Accelerator,
        batch_size: int,
        **dataloader_kwargs: dict[str, Any],
    ):
        self.dataset = dataset
        self.accelerator = accelerator
        self.batch_size = batch_size
        self.dataloader_kwargs = dataloader_kwargs

    def __len__(self):
        return self.dataset.estimate_num_batches(self.batch_size)

    def generate(self) -> torch.Tensor:
        # Iterate through each realization in our dataset
        random.shuffle(self.dataset.realizations)

        for realization in self.dataset.realizations:
            # Load a realization of data into memory
            self.dataset.load_data(realization)

            # Wrap a dataloader around it and generate the data
            dl = self.accelerator.prepare(
                DataLoader(
                    self.dataset, batch_size=self.batch_size, **self.dataloader_kwargs
                )
            )

            for sample in dl:
                yield sample